üß† Offline Edge RAG: React NativeA 100% offline, privacy-first Retrieval-Augmented Generation (RAG) system built entirely on-device for React Native.üìñ OverviewMost AI applications are just API wrappers. This project is different. It is a fully self-contained RAG pipeline running natively on mobile hardware. It ingests documents, chunks them, generates high-dimensional embeddings, and performs semantic vector searches to augment a local LLM‚Äîall without a single network request after the initial model download.Developed by Nitesh Domal as an exploration of pushing Edge AI to its limits on mobile architectures.‚ú® Key FeaturesZero API Dependency: No OpenAI, no Pinecone, no cloud costs. Absolute data privacy.Dual-Model Architecture: Simultaneously runs a local LLM for text generation and a separate embedding model for vector mapping.Custom JS Vector Database: Bypasses heavy native SQLite dependencies with a lightweight, flat-file JSON vector index.On-Device Chunking & Ingestion: Reads .txt files directly from the mobile file system, utilizing RecursiveCharacterTextSplitter to optimize context windows.üõ†Ô∏è The Tech StackFramework: React Native (0.73+)Inference Engine: llama.rn (C++ bindings for llama.cpp)LLM: Qwen 2.5 (0.5B Parameters, Q4_K_M GGUF) - Chosen for sub-500MB memory footprint.Embeddings: Nomic Embed Text v1.5 (GGUF) - High-performance, low-latency semantic search.Math/Vector Search: Pure JavaScript execution.‚öôÔ∏è Engineering Challenges & ArchitectureRunning heavy machine learning tasks on a mobile device requires ruthless optimization. This app was successfully benchmarked on mid-range Android hardware (Vivo 1804), requiring strict memory and UI thread management.1. The Vector Search BottleneckInstead of relying on heavy C++ SQLite vector extensions, I engineered a custom flat-file database using a Brute Force $O(N)$ vector search. The mathematical core relies on calculating Cosine Similarity natively within the JS thread:$$similarity = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}$$2. Context Window ManagementMobile LLMs have strict context limits. By dynamically shrinking the chunkSize to 500 characters and sorting the vector matches by highest cosine similarity, the system safely fits the system prompt, historical chat context, and top 2-3 data chunks inside Qwen 2.5's 2048 token limit without crashing the OS.3. RAM HeadroomLoading two .gguf models simultaneously into unified mobile memory is highly volatile. By downsizing to the 0.5B model and strictly defining use_mlock: true, the application footprint stays under ~600MB, leaving enough headroom for the React Native UI thread to render animations smoothly at 60fps.üöÄ Quick StartClone the repo:Bashgit clone https://github.com/yourusername/offline-edge-rag-rn.git
cd offline-edge-rag-rn
Install dependencies:Bashnpm install
# or
yarn install
Run the app:Bashnpx react-native run-android
# or
npx react-native run-ios
Note: On first launch, the app will download the necessary .gguf models directly to the device's document directory.
